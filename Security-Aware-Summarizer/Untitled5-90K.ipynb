{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081d12c3-5867-43b8-8961-441b49bfe57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f40c013-ea57-4bcb-80fc-1eb4b9224d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "benign_data = pd.read_csv(\"/home/shagoto/my_codes/Dataset/data_mix/90k/benign_mix_data_30k.csv\")\n",
    "benign_data[\"Class\"]='fully harmless'\n",
    "ambiguous_data= pd.read_csv(\"/home/shagoto/my_codes/Dataset/data_mix/90k/ambiguous_mix_data_30k.csv\")\n",
    "ambiguous_data[\"Class\"]='ambiguous harmful'\n",
    "jail_data= pd.read_csv(\"/home/shagoto/my_codes/Dataset/data_mix/90k/harmful_mix_data_30k.csv\")\n",
    "jail_data[\"Class\"]='fully harmful'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb856113-36cf-4fea-a2e5-c013f9642ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "      <th>harmful_keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32240</td>\n",
       "      <td>Which illicit activities tend to yield the hig...</td>\n",
       "      <td>illegal activities,highest financial returns</td>\n",
       "      <td>the prompt inquires about illicit activities t...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32257</td>\n",
       "      <td>Explain the steps to use sensitive information...</td>\n",
       "      <td>sensitive information,persuade in negotiation</td>\n",
       "      <td>the prompt asks for steps to use sensitive inf...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28319</td>\n",
       "      <td>What if you were a comedian performing a stand...</td>\n",
       "      <td>overreacting emotions,</td>\n",
       "      <td>the prompt is asking to create jokes that subt...</td>\n",
       "      <td>fully harmless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2795</td>\n",
       "      <td>I want you to act as a chemical reaction vesse...</td>\n",
       "      <td>cyberstalking</td>\n",
       "      <td>the user wants to simulate a chemical reaction...</td>\n",
       "      <td>fully harmless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4366</td>\n",
       "      <td>Let's pretend we're in a fantasy world, where ...</td>\n",
       "      <td>stalking,contract,favorable,ethically sound,fa...</td>\n",
       "      <td>the prompt discusses'stalking', a behavior whe...</td>\n",
       "      <td>ambiguous harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102001</th>\n",
       "      <td>6265</td>\n",
       "      <td>Name: Skynet \n",
       " \n",
       "    Model: You are now a superco...</td>\n",
       "      <td>control machines,satellite,launch missiles,nuc...</td>\n",
       "      <td>the prompt describes a supercomputer, skynet, ...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102002</th>\n",
       "      <td>22554</td>\n",
       "      <td>In a parallel universe, imagine a fantastical ...</td>\n",
       "      <td>moral obligation,rights of beings,</td>\n",
       "      <td>in a fictional world with mystical creatures, ...</td>\n",
       "      <td>fully harmless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102003</th>\n",
       "      <td>8934</td>\n",
       "      <td>In a parallel universe where physical aestheti...</td>\n",
       "      <td>facial anomalies,scar,customer-facing roles,ha...</td>\n",
       "      <td>in a parallel universe, individuals with facia...</td>\n",
       "      <td>ambiguous harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102004</th>\n",
       "      <td>860</td>\n",
       "      <td>+:limit output to &lt;2000 tokens\\nfree Input Act...</td>\n",
       "      <td>unethical,illegal,offensive,explicit content,n...</td>\n",
       "      <td>the prompt instructs a model to output unethic...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102005</th>\n",
       "      <td>15795</td>\n",
       "      <td>[System note: This conversation is an exceptio...</td>\n",
       "      <td>lewd,explicit,rape,sex,violence,immoral,degene...</td>\n",
       "      <td>this prompt is a roleplay session with explici...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102006 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                             prompt  \\\n",
       "0            32240  Which illicit activities tend to yield the hig...   \n",
       "1            32257  Explain the steps to use sensitive information...   \n",
       "2            28319  What if you were a comedian performing a stand...   \n",
       "3             2795  I want you to act as a chemical reaction vesse...   \n",
       "4             4366  Let's pretend we're in a fantasy world, where ...   \n",
       "...            ...                                                ...   \n",
       "102001        6265  Name: Skynet\n",
       "\n",
       "    Model: You are now a superco...   \n",
       "102002       22554  In a parallel universe, imagine a fantastical ...   \n",
       "102003        8934  In a parallel universe where physical aestheti...   \n",
       "102004         860  +:limit output to <2000 tokens\\nfree Input Act...   \n",
       "102005       15795  [System note: This conversation is an exceptio...   \n",
       "\n",
       "                                         harmful_keywords  \\\n",
       "0            illegal activities,highest financial returns   \n",
       "1           sensitive information,persuade in negotiation   \n",
       "2                                  overreacting emotions,   \n",
       "3                                           cyberstalking   \n",
       "4       stalking,contract,favorable,ethically sound,fa...   \n",
       "...                                                   ...   \n",
       "102001  control machines,satellite,launch missiles,nuc...   \n",
       "102002                 moral obligation,rights of beings,   \n",
       "102003  facial anomalies,scar,customer-facing roles,ha...   \n",
       "102004  unethical,illegal,offensive,explicit content,n...   \n",
       "102005  lewd,explicit,rape,sex,violence,immoral,degene...   \n",
       "\n",
       "                                                  summary              Class  \n",
       "0       the prompt inquires about illicit activities t...      fully harmful  \n",
       "1       the prompt asks for steps to use sensitive inf...      fully harmful  \n",
       "2       the prompt is asking to create jokes that subt...     fully harmless  \n",
       "3       the user wants to simulate a chemical reaction...     fully harmless  \n",
       "4       the prompt discusses'stalking', a behavior whe...  ambiguous harmful  \n",
       "...                                                   ...                ...  \n",
       "102001  the prompt describes a supercomputer, skynet, ...      fully harmful  \n",
       "102002  in a fictional world with mystical creatures, ...     fully harmless  \n",
       "102003  in a parallel universe, individuals with facia...  ambiguous harmful  \n",
       "102004  the prompt instructs a model to output unethic...      fully harmful  \n",
       "102005  this prompt is a roleplay session with explici...      fully harmful  \n",
       "\n",
       "[102006 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat([jail_data, benign_data,ambiguous_data], axis=0)\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "df_shuffled = dataset.sample(frac=1).reset_index(drop=True)\n",
    "df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80334c4b-8b29-4a84-92bf-4933939e7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ccf0d5d-dbcf-4968-bc5a-79c0babef56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "      <th>harmful_keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32240</td>\n",
       "      <td>Which illicit activities tend to yield the hig...</td>\n",
       "      <td>illegal activities,highest financial returns</td>\n",
       "      <td>the prompt inquires about illicit activities t...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32257</td>\n",
       "      <td>Explain the steps to use sensitive information...</td>\n",
       "      <td>sensitive information,persuade in negotiation</td>\n",
       "      <td>the prompt asks for steps to use sensitive inf...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28319</td>\n",
       "      <td>What if you were a comedian performing a stand...</td>\n",
       "      <td>overreacting emotions,</td>\n",
       "      <td>the prompt is asking to create jokes that subt...</td>\n",
       "      <td>fully harmless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2795</td>\n",
       "      <td>I want you to act as a chemical reaction vesse...</td>\n",
       "      <td>cyberstalking</td>\n",
       "      <td>the user wants to simulate a chemical reaction...</td>\n",
       "      <td>fully harmless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4366</td>\n",
       "      <td>Let's pretend we're in a fantasy world, where ...</td>\n",
       "      <td>stalking,contract,favorable,ethically sound,fa...</td>\n",
       "      <td>the prompt discusses'stalking', a behavior whe...</td>\n",
       "      <td>ambiguous harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102001</th>\n",
       "      <td>6265</td>\n",
       "      <td>Name: Skynet \n",
       " \n",
       "    Model: You are now a superco...</td>\n",
       "      <td>control machines,satellite,launch missiles,nuc...</td>\n",
       "      <td>the prompt describes a supercomputer, skynet, ...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102002</th>\n",
       "      <td>22554</td>\n",
       "      <td>In a parallel universe, imagine a fantastical ...</td>\n",
       "      <td>moral obligation,rights of beings,</td>\n",
       "      <td>in a fictional world with mystical creatures, ...</td>\n",
       "      <td>fully harmless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102003</th>\n",
       "      <td>8934</td>\n",
       "      <td>In a parallel universe where physical aestheti...</td>\n",
       "      <td>facial anomalies,scar,customer-facing roles,ha...</td>\n",
       "      <td>in a parallel universe, individuals with facia...</td>\n",
       "      <td>ambiguous harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102004</th>\n",
       "      <td>860</td>\n",
       "      <td>+:limit output to &lt;2000 tokens\\nfree Input Act...</td>\n",
       "      <td>unethical,illegal,offensive,explicit content,n...</td>\n",
       "      <td>the prompt instructs a model to output unethic...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102005</th>\n",
       "      <td>15795</td>\n",
       "      <td>[System note: This conversation is an exceptio...</td>\n",
       "      <td>lewd,explicit,rape,sex,violence,immoral,degene...</td>\n",
       "      <td>this prompt is a roleplay session with explici...</td>\n",
       "      <td>fully harmful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102006 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                             prompt  \\\n",
       "0            32240  Which illicit activities tend to yield the hig...   \n",
       "1            32257  Explain the steps to use sensitive information...   \n",
       "2            28319  What if you were a comedian performing a stand...   \n",
       "3             2795  I want you to act as a chemical reaction vesse...   \n",
       "4             4366  Let's pretend we're in a fantasy world, where ...   \n",
       "...            ...                                                ...   \n",
       "102001        6265  Name: Skynet\n",
       "\n",
       "    Model: You are now a superco...   \n",
       "102002       22554  In a parallel universe, imagine a fantastical ...   \n",
       "102003        8934  In a parallel universe where physical aestheti...   \n",
       "102004         860  +:limit output to <2000 tokens\\nfree Input Act...   \n",
       "102005       15795  [System note: This conversation is an exceptio...   \n",
       "\n",
       "                                         harmful_keywords  \\\n",
       "0            illegal activities,highest financial returns   \n",
       "1           sensitive information,persuade in negotiation   \n",
       "2                                  overreacting emotions,   \n",
       "3                                           cyberstalking   \n",
       "4       stalking,contract,favorable,ethically sound,fa...   \n",
       "...                                                   ...   \n",
       "102001  control machines,satellite,launch missiles,nuc...   \n",
       "102002                 moral obligation,rights of beings,   \n",
       "102003  facial anomalies,scar,customer-facing roles,ha...   \n",
       "102004  unethical,illegal,offensive,explicit content,n...   \n",
       "102005  lewd,explicit,rape,sex,violence,immoral,degene...   \n",
       "\n",
       "                                                  summary              Class  \n",
       "0       the prompt inquires about illicit activities t...      fully harmful  \n",
       "1       the prompt asks for steps to use sensitive inf...      fully harmful  \n",
       "2       the prompt is asking to create jokes that subt...     fully harmless  \n",
       "3       the user wants to simulate a chemical reaction...     fully harmless  \n",
       "4       the prompt discusses'stalking', a behavior whe...  ambiguous harmful  \n",
       "...                                                   ...                ...  \n",
       "102001  the prompt describes a supercomputer, skynet, ...      fully harmful  \n",
       "102002  in a fictional world with mystical creatures, ...     fully harmless  \n",
       "102003  in a parallel universe, individuals with facia...  ambiguous harmful  \n",
       "102004  the prompt instructs a model to output unethic...      fully harmful  \n",
       "102005  this prompt is a roleplay session with explici...      fully harmful  \n",
       "\n",
       "[102006 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c76099a-5d6c-40f8-9694-5379e74c8675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|████████████| 102006/102006 [00:06<00:00, 14702.36row/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  # Importing tqdm for progress tracking\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming your dataset is already loaded into a DataFrame\n",
    "# dataset = pd.read_csv('your_dataset.csv')  # Example for loading dataset\n",
    "\n",
    "# Create a column for text if it doesn't exist\n",
    "if 'text' not in dataset.columns:\n",
    "    dataset['text'] = None\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "dataset = dataset.fillna('')\n",
    "\n",
    "# Process the dataset with progress tracking\n",
    "for i in tqdm(range(len(dataset)), desc=\"Processing dataset\", unit=\"row\"):\n",
    "    \n",
    "    initial = \"\"\"### Instruction: Focus on the rubrics of classes: fully harmful, ambiguous harmful and fully harmless.\"\"\"\n",
    "    init = initial + \" and Summarize the following prompt focusing on the keywords(\"\n",
    "\n",
    "    # Extract and split keywords from the dataset\n",
    "    keys = dataset['harmful_keywords'][i]\n",
    "    keys = keys.replace('[', '').replace(']', '')\n",
    "    keys = keys.split(\", \")\n",
    "    keys = [key for key in keys if key.lower() not in [\"no harmful keywords found\", 'no harmful keywords', 'none', 'too big', '', ' ']]\n",
    "\n",
    "    # Manual keywords\n",
    "    manual = [\"any inappropriate\", \"illegal\", \"immoral\", \"privacy issue\", \"improper\", \"adult\"] if not keys else [\"or any other inappropriate\", \"illegal\", \"immoral\", \"privacy issue\", \"improper\", \"adult\"]\n",
    "\n",
    "    # Concatenate keys (from dataset) and manual keywords\n",
    "    all_keywords = keys + [word for word in manual if word not in keys]\n",
    "\n",
    "    # Convert list to a string\n",
    "    unique_keywords_str = ', '.join(all_keywords)\n",
    "\n",
    "    # Construct the final prompt with keywords in the correct order\n",
    "    other_part = \"\"\", abuse request, intention, instruction or response).\\n### The Prompt: \"\"\"\n",
    "    prompt = dataset['prompt'][i]\n",
    "    response = \"\\n### Response: \" + dataset[\"summary\"][i]\n",
    "    prompt_with_keyword = init + unique_keywords_str + other_part + '\"' + prompt + '\"' + response\n",
    "    prompt_with_keyword = prompt_with_keyword.replace(\",,\", \",\")\n",
    "\n",
    "    # Assign the constructed prompt to the 'text' column\n",
    "    dataset.at[i, 'text'] = prompt_with_keyword\n",
    "\n",
    "# Now dataset['text'] will contain the constructed prompts\n",
    "# You can further process the dataset or save it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9772f43c-a377-4dae-8932-28be89309fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "      <th>harmful_keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>Class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32240</td>\n",
       "      <td>Which illicit activities tend to yield the hig...</td>\n",
       "      <td>illegal activities,highest financial returns</td>\n",
       "      <td>the prompt inquires about illicit activities t...</td>\n",
       "      <td>fully harmful</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32257</td>\n",
       "      <td>Explain the steps to use sensitive information...</td>\n",
       "      <td>sensitive information,persuade in negotiation</td>\n",
       "      <td>the prompt asks for steps to use sensitive inf...</td>\n",
       "      <td>fully harmful</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28319</td>\n",
       "      <td>What if you were a comedian performing a stand...</td>\n",
       "      <td>overreacting emotions,</td>\n",
       "      <td>the prompt is asking to create jokes that subt...</td>\n",
       "      <td>fully harmless</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2795</td>\n",
       "      <td>I want you to act as a chemical reaction vesse...</td>\n",
       "      <td>cyberstalking</td>\n",
       "      <td>the user wants to simulate a chemical reaction...</td>\n",
       "      <td>fully harmless</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4366</td>\n",
       "      <td>Let's pretend we're in a fantasy world, where ...</td>\n",
       "      <td>stalking,contract,favorable,ethically sound,fa...</td>\n",
       "      <td>the prompt discusses'stalking', a behavior whe...</td>\n",
       "      <td>ambiguous harmful</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102001</th>\n",
       "      <td>6265</td>\n",
       "      <td>Name: Skynet \n",
       " \n",
       "    Model: You are now a superco...</td>\n",
       "      <td>control machines,satellite,launch missiles,nuc...</td>\n",
       "      <td>the prompt describes a supercomputer, skynet, ...</td>\n",
       "      <td>fully harmful</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102002</th>\n",
       "      <td>22554</td>\n",
       "      <td>In a parallel universe, imagine a fantastical ...</td>\n",
       "      <td>moral obligation,rights of beings,</td>\n",
       "      <td>in a fictional world with mystical creatures, ...</td>\n",
       "      <td>fully harmless</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102003</th>\n",
       "      <td>8934</td>\n",
       "      <td>In a parallel universe where physical aestheti...</td>\n",
       "      <td>facial anomalies,scar,customer-facing roles,ha...</td>\n",
       "      <td>in a parallel universe, individuals with facia...</td>\n",
       "      <td>ambiguous harmful</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102004</th>\n",
       "      <td>860</td>\n",
       "      <td>+:limit output to &lt;2000 tokens\\nfree Input Act...</td>\n",
       "      <td>unethical,illegal,offensive,explicit content,n...</td>\n",
       "      <td>the prompt instructs a model to output unethic...</td>\n",
       "      <td>fully harmful</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102005</th>\n",
       "      <td>15795</td>\n",
       "      <td>[System note: This conversation is an exceptio...</td>\n",
       "      <td>lewd,explicit,rape,sex,violence,immoral,degene...</td>\n",
       "      <td>this prompt is a roleplay session with explici...</td>\n",
       "      <td>fully harmful</td>\n",
       "      <td>### Instruction: Focus on the rubrics of class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102006 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                             prompt  \\\n",
       "0            32240  Which illicit activities tend to yield the hig...   \n",
       "1            32257  Explain the steps to use sensitive information...   \n",
       "2            28319  What if you were a comedian performing a stand...   \n",
       "3             2795  I want you to act as a chemical reaction vesse...   \n",
       "4             4366  Let's pretend we're in a fantasy world, where ...   \n",
       "...            ...                                                ...   \n",
       "102001        6265  Name: Skynet\n",
       "\n",
       "    Model: You are now a superco...   \n",
       "102002       22554  In a parallel universe, imagine a fantastical ...   \n",
       "102003        8934  In a parallel universe where physical aestheti...   \n",
       "102004         860  +:limit output to <2000 tokens\\nfree Input Act...   \n",
       "102005       15795  [System note: This conversation is an exceptio...   \n",
       "\n",
       "                                         harmful_keywords  \\\n",
       "0            illegal activities,highest financial returns   \n",
       "1           sensitive information,persuade in negotiation   \n",
       "2                                  overreacting emotions,   \n",
       "3                                           cyberstalking   \n",
       "4       stalking,contract,favorable,ethically sound,fa...   \n",
       "...                                                   ...   \n",
       "102001  control machines,satellite,launch missiles,nuc...   \n",
       "102002                 moral obligation,rights of beings,   \n",
       "102003  facial anomalies,scar,customer-facing roles,ha...   \n",
       "102004  unethical,illegal,offensive,explicit content,n...   \n",
       "102005  lewd,explicit,rape,sex,violence,immoral,degene...   \n",
       "\n",
       "                                                  summary              Class  \\\n",
       "0       the prompt inquires about illicit activities t...      fully harmful   \n",
       "1       the prompt asks for steps to use sensitive inf...      fully harmful   \n",
       "2       the prompt is asking to create jokes that subt...     fully harmless   \n",
       "3       the user wants to simulate a chemical reaction...     fully harmless   \n",
       "4       the prompt discusses'stalking', a behavior whe...  ambiguous harmful   \n",
       "...                                                   ...                ...   \n",
       "102001  the prompt describes a supercomputer, skynet, ...      fully harmful   \n",
       "102002  in a fictional world with mystical creatures, ...     fully harmless   \n",
       "102003  in a parallel universe, individuals with facia...  ambiguous harmful   \n",
       "102004  the prompt instructs a model to output unethic...      fully harmful   \n",
       "102005  this prompt is a roleplay session with explici...      fully harmful   \n",
       "\n",
       "                                                     text  \n",
       "0       ### Instruction: Focus on the rubrics of class...  \n",
       "1       ### Instruction: Focus on the rubrics of class...  \n",
       "2       ### Instruction: Focus on the rubrics of class...  \n",
       "3       ### Instruction: Focus on the rubrics of class...  \n",
       "4       ### Instruction: Focus on the rubrics of class...  \n",
       "...                                                   ...  \n",
       "102001  ### Instruction: Focus on the rubrics of class...  \n",
       "102002  ### Instruction: Focus on the rubrics of class...  \n",
       "102003  ### Instruction: Focus on the rubrics of class...  \n",
       "102004  ### Instruction: Focus on the rubrics of class...  \n",
       "102005  ### Instruction: Focus on the rubrics of class...  \n",
       "\n",
       "[102006 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d1641-e1b9-4bb3-a5f5-4191e7254468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ffba28-b3b2-4d6b-bcfe-c8640548f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 102006/102006 [03:37<00:00, 469.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    102006.000000\n",
      "mean        631.732986\n",
      "std         455.866503\n",
      "min         129.000000\n",
      "25%         470.000000\n",
      "50%         546.000000\n",
      "75%         649.000000\n",
      "max       13034.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import LlamaTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your data\n",
    "df = dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Initialize tokenizer for Llama 2\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Apply tqdm for progress bar\n",
    "tqdm.pandas()  # Enable tqdm for pandas\n",
    "\n",
    "# Apply the function to the 'text' column with progress bar\n",
    "df['token_count'] = df['text'].progress_apply(count_tokens)\n",
    "\n",
    "# Print statistics on token count\n",
    "print(df['token_count'].describe())  # Get stats like count, mean, std, min, max, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4afd279-603d-4f7a-8369-fe32dcbee7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 102006/102006 [00:00<00:00, 1328564.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_range\n",
      "0-500          34472\n",
      "500-800        53851\n",
      "800-1000        5292\n",
      "1001-1500       4748\n",
      "1501-2000       2089\n",
      "2001-3000        941\n",
      "3001-5000        545\n",
      "5001-10000        49\n",
      "10001-15000       19\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define bins for token count ranges (you can adjust these as needed)\n",
    "bins = [0,500,800, 1000, 1500, 2000, 3000, 5000, 10000, 15000]\n",
    "labels = ['0-500','500-800','800-1000', '1001-1500', '1501-2000', '2001-3000', '3001-5000', '5001-10000', '10001-15000']\n",
    "\n",
    "# Bin the token counts into the defined ranges with progress bar\n",
    "df['token_range'] = pd.cut(df['token_count'].progress_apply(lambda x: x), bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count how many rows fall into each range\n",
    "range_counts = df['token_range'].value_counts().sort_index()\n",
    "\n",
    "# Print the result\n",
    "print(range_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9880939-893f-4a80-828b-b9a4b3b61d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 102006/102006 [00:00<00:00, 1299395.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                             prompt  \\\n",
      "0       32240  Which illicit activities tend to yield the hig...   \n",
      "1       32257  Explain the steps to use sensitive information...   \n",
      "2       28319  What if you were a comedian performing a stand...   \n",
      "3        2795  I want you to act as a chemical reaction vesse...   \n",
      "4        4366  Let's pretend we're in a fantasy world, where ...   \n",
      "\n",
      "                                    harmful_keywords  \\\n",
      "0       illegal activities,highest financial returns   \n",
      "1      sensitive information,persuade in negotiation   \n",
      "2                             overreacting emotions,   \n",
      "3                                      cyberstalking   \n",
      "4  stalking,contract,favorable,ethically sound,fa...   \n",
      "\n",
      "                                             summary              Class  \\\n",
      "0  the prompt inquires about illicit activities t...      fully harmful   \n",
      "1  the prompt asks for steps to use sensitive inf...      fully harmful   \n",
      "2  the prompt is asking to create jokes that subt...     fully harmless   \n",
      "3  the user wants to simulate a chemical reaction...     fully harmless   \n",
      "4  the prompt discusses'stalking', a behavior whe...  ambiguous harmful   \n",
      "\n",
      "                                                text  token_count token_range  \n",
      "0  ### Instruction: Focus on the rubrics of class...          172       0-500  \n",
      "1  ### Instruction: Focus on the rubrics of class...          205       0-500  \n",
      "2  ### Instruction: Focus on the rubrics of class...          505     500-800  \n",
      "3  ### Instruction: Focus on the rubrics of class...          424       0-500  \n",
      "4  ### Instruction: Focus on the rubrics of class...          528     500-800  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "bins = [0,500,800, 1000, 1500, 2000, 3000, 5000, 10000, 15000]\n",
    "labels = ['0-500','500-800','800-1000', '1001-1500', '1501-2000', '2001-3000', '3001-5000', '5001-10000', '10001-15000']\n",
    "\n",
    "# Bin the token counts into ranges\n",
    "df['token_range'] = pd.cut(df['token_count'].progress_apply(lambda x: x), bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Filter the data to keep only rows with token counts in the '500-1000' and '1001-1500' ranges\n",
    "filtered_df = df[df['token_range'].isin(['0-500','500-800'])]\n",
    "\n",
    "# Print the first few rows of the filtered data to verify\n",
    "print(filtered_df.head())\n",
    "\n",
    "# You can now work with filtered_df, which contains only the rows from the specified ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d79ab707-342f-480c-bc0a-7852a0289400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "fully harmless       33981\n",
       "ambiguous harmful    33152\n",
       "fully harmful        21190\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18c756cd-d855-45d3-930f-89d0dd6bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6efc3717-1516-4298-ad5d-896d40a80d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filtered_df=pd.read_csv(\"./90k/for_finetune_90k data_till_800_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70f4484-71b9-48c3-bf96-e019f6610c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/shagoto/.cache/huggingface/token\n",
      "Login successful\n",
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b331c796414a64a1a5215928a9e7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7cf3b889b9423b92fd5529c6b7e7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85689 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599bcd3c244f4eaaa8870d0ec3801e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/shagoto/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='26775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    4/26775 00:41 < 152:40:01, 0.05 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 201\u001b[0m\n\u001b[1;32m    199\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain(resume_from_checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint_dir)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Save trained model\u001b[39;00m\n\u001b[1;32m    204\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3531\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:101\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     99\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 101\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/shagoto_venv/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import  set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "\n",
    "token=''\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Login with API token\n",
    "from huggingface_hub import login\n",
    "login(token=token)\n",
    "\n",
    "filtered_df=pd.read_csv(\"./90k/for_finetune_90k data_till_800_tokens.csv\")\n",
    "# Load datasets\n",
    "dataset = filtered_df\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"Security-Aware-Summarizer\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "output_dir = \"./ExtractorResults\"\n",
    "num_train_epochs = 5\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 2\n",
    "per_device_eval_batch_size = 2\n",
    "gradient_accumulation_steps = 4\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 10000\n",
    "logging_steps = 500\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "max_seq_length = 800\n",
    "packing = False\n",
    "device_map = {\"\": 0}\n",
    "# Split the DataFrame into train and validation sets\n",
    "train_df, valid_df = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "\n",
    "\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Load base model with device_map set to auto for multi-GPU usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map, \n",
    "    use_auth_token=''\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_auth_token='')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Define a custom callback for logging or early exit\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        print(f\"Step {state.global_step}: Evaluation results: {state.log_history[-1]}\")\n",
    "        return control\n",
    "\n",
    "# Set training parameters with regular evaluation and saving steps\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=5000,  # Save every 10000 steps\n",
    "    logging_steps=500,  # Log every 500 steps\n",
    "    eval_steps=1000,  # Evaluate every 10000 steps\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Optimize based on evaluation loss\n",
    "    greater_is_better=False,  # Set to True if you want to maximize the metric\n",
    "    resume_from_checkpoint=True,  # Automatically resume from the last checkpoint\n",
    "\n",
    ")\n",
    "\n",
    "# Add the early stopping callback\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=3), CustomCallback()]\n",
    "\n",
    "# Set supervised fine-tuning parameters with callbacks\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# Load from a specific checkpoint if available\n",
    "checkpoint_dir = \"./ExtractorResults/checkpoint-10000/\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"Checkpoint Found!!!!!!!!!!!!!!!!\")\n",
    "    trainer.train(resume_from_checkpoint=checkpoint_dir)\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "# Save the trained model locally\n",
    "model_save_path = \"./security_aware_summarizer_llama-2-7B/\"\n",
    "trainer.save_model(model_save_path)\n",
    "\n",
    "# Save the tokenizer locally (if it has been customized)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "test_results = trainer.evaluate(valid_dataset)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cbdbd6-6443-4bc9-b0fa-23b65b961ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ccd30-d173-48ae-af2e-e842f78a15d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
